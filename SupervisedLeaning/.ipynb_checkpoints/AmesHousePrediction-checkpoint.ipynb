{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import linear_model as ln\n",
    "\n",
    "data=pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#histogram\n",
    "sns.distplot(data['SalePrice']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data['SalePrice'])\n",
    "plt.show()\n",
    "print (\"mean = %lf\",data['SalePrice'].mean())\n",
    "print (\"std = %lf\" ,data['SalePrice'].std())\n",
    "print (\"median = %lf\" ,data['SalePrice'].median())\n",
    "print(\"Skewness: %f\" , data['SalePrice'].skew())\n",
    "print(\"Kurtosis: %f\" , data['SalePrice'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, it is clear that we could have to deal with a certain number of outliers. Let us firsrt looking for potential relationships between the different numerical/categorical inputs and the output target variable (SalePrice).\n",
    "\n",
    "### Numerical features\n",
    "\n",
    "Let us start analyzing numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This command allows you to easily determine the numerical variables\n",
    "# In this way, we can easily produces a series of scatter plots, useful for visually detecting some potential correlation\n",
    "numerical=data._get_numeric_data().columns\n",
    "numerical=numerical.drop('SalePrice')\n",
    "numerical=numerical.drop('Id')\n",
    "In [8]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target='SalePrice'\n",
    "Y=data[target].values\n",
    "for x in numerical:\n",
    "    X=data[x].values\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(target)\n",
    "    plt.scatter(X,Y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features show a quite clear linear relation with the target variable 'SalePrice', while others not, due to the amount of noisyness that characterize them. Moreover it seems that some of this input features contain a quite small number of observations (i.e. many missing values), and then could be of no use for our regression study.\n",
    "\n",
    "### Categorical features\n",
    "\n",
    "Now, let us have a look at categorical features: seaborn allow us to easily produce bar charts plots that could reveal some interesting relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical=data.columns.drop(numerical).drop(target).drop('Id')\n",
    "for i in categorical:\n",
    "    \n",
    "    g=sns.boxplot(x=i, y=target, data=data[[i,target]]);\n",
    "    for item in g.get_xticklabels():\n",
    "        item.set_rotation(60)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be some interesting relation between our target variable and several categorical and numerical features.\n",
    "\n",
    "Before applying a regression model, it is better to look for some potential collinearity among our numerical features: this can be easily done computing a matrix of correlation coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "corrmat = data.corr()#(method='spearman')\n",
    "k = 20 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index # takes the first k entries with largest correlation\n",
    "cm = np.corrcoef(data[cols].values.T)\n",
    "sns.set(font_scale=1.15)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, there it seems to be some collinearity between our input variables: this tells us that we will have to use regression regularized regression algorithms in order to have a properly performing learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keeping only features which show (even if weakly) a correlation \n",
    "# with our target variable. I am not sure how much solid this approach could be.\n",
    "\n",
    "threshold=.3 \n",
    "rel=[]\n",
    "relevant=[]\n",
    "for x in cols.drop(target):\n",
    "    coeff=np.corrcoef(data[[x,target]].values.T)[1,0]\n",
    "    if coeff > threshold:\n",
    "         rel.append([x,coeff])\n",
    "         relevant.append(x)   \n",
    "rel.sort(key=lambda x: x[1])\n",
    "relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us just give a look at missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#missing data\n",
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way of proceeding is to eliminate those features whose missing percentage is more than 15%. For the remaining features, missing values are filled using substitution by imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr=.15\n",
    "for j in missing_data.index.values:\n",
    "     if missing_data['Percent'].loc[j]>tr:\n",
    "            data=data.drop(j, 1)\n",
    "            if j in relevant:\n",
    "                relevant.remove(j)\n",
    "relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the relevant categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numerical=data._get_numeric_data().columns\n",
    "categorical=data.columns.drop(numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURES ENGINEERING\n",
    "After having collected all the potentially interesting features, we need to subsitute missing values and apply log transformations on both target and input variables, followed by a standardization of the latter (this should enhance the convergence of our training algorithms). The dummy variables need to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "#applying log transformation on target variable\n",
    "train=data\n",
    "train['SalePrice'] = np.log(train['SalePrice'])\n",
    "\n",
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(train['SalePrice'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transformation on target variable works pretty well: the skewness has been quite reduced, and it seems we should not worry about outliers(for the moment).\n",
    "\n",
    "Now, let's have a look at our numerical input features, fill their missing values, and see if some of them could benefit from the application of a log-transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from scipy.stats import norm\n",
    "\n",
    "imput=Imputer(missing_values='NaN', strategy='mean')\n",
    "imput.fit_transform(train[relevant])\n",
    "train[relevant]=imput.transform(train[relevant])\n",
    "\n",
    "Y=data[target].values\n",
    "for x in relevant:\n",
    "    if train[x].min()>0:\n",
    "        X=np.log(train[x]).values\n",
    "        print train[x].describe()\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(target)\n",
    "        plt.scatter(X,Y)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the log-transformation only for those features whose minimum value is not zero. From the plots above it seems that log-transforming candidate numerical features could help in having enough homoscedasticity, which is one of the fundamental requirements for applying linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_log=[]\n",
    "for x in relevant:\n",
    "    if train[x].min()!=0:\n",
    "        to_log.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[to_log]=np.log(train[to_log])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply standardization, in order to have predictors centered around zero, with a common range scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as ppr\n",
    "\n",
    "\n",
    "stand=ppr.StandardScaler()\n",
    "train[relevant]=stand.fit_transform(train[relevant])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to add dummy variables: at this point it quit important to first investigate the shapes of train and test set, in order to understand which feature can be retained and which not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv('../input/test.csv')\n",
    "test[relevant].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_catg=[]\n",
    "for i in categorical:\n",
    "    dummy1=pd.get_dummies(train[i]).shape[1]\n",
    "    dummy2=pd.get_dummies(test[i]).shape[1]\n",
    "    if dummy1==dummy2:\n",
    "        new_catg.append(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names included in \"new_catg\" represents categories which mantain the same cardinality from train to test set, hence can be used for prediction. Finally, we create our final train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=pd.concat([train[relevant],pd.get_dummies(train[new_catg])],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with the training of the learners, we already transform the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imput=Imputer(missing_values='NaN', strategy='mean')\n",
    "imput.fit_transform(test[relevant])\n",
    "test[relevant]=imput.transform(test[relevant])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply log transformation and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[to_log]=np.log(test[to_log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as ppr\n",
    "\n",
    "stand=ppr.StandardScaler()\n",
    "test[relevant]=stand.fit_transform(test[relevant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest=pd.concat([test[relevant],pd.get_dummies(test[new_catg])],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now training and test set are ready, and we can focus on the implementation of learning algorithms.\n",
    "\n",
    "\n",
    "#### MODEL SELECTION AND OPTIMIZATION\n",
    "Here we are! This is the most (in my opinion) exciting part of a data science project: model selection and optimization. Here we focus on different types of regression learners that could be used for predicting the SalePrice variable. Each learner comes with its specific strength and weak point, and a certain number of hyper parameters that need to be trained in order to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys=[\"alpha\",\"intercept\",\"coefficients\"]\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso: parameter tuning¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LASSOdic=dict.fromkeys(keys,None)\n",
    "y=train[target]\n",
    "print(\"Computing regularization path using the coordinate descent lasso...\")\n",
    "t1 = time.time()\n",
    "model = ln.LassoCV(cv=5).fit(X, y)\n",
    "t_lasso_cv = time.time() - t1\n",
    "\n",
    "# Display results\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "#ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: CV estimate')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.title('Mean square error on each fold: coordinate descent '\n",
    "          '(train time: %.2fs)' % t_lasso_cv)\n",
    "plt.axis('tight')\n",
    "\n",
    "LASSOdic[\"coefficients\"]=model.coef_\n",
    "LASSOdic[\"alpha\"]=model.alpha_\n",
    "LASSOdic[\"intercept\"]=model.intercept_\n",
    "\n",
    "LASSOdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfLA=ln.Lasso(alpha=LASSOdic[\"alpha\"])\n",
    "clfLA.fit(X,y)\n",
    "clfLA.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge: parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RIDGEdic=dict.fromkeys(keys,None)\n",
    "print(\"Computing regularization path using the coordinate descent Ridge...\")\n",
    "t1 = time.time()\n",
    "model = ln.RidgeCV(cv=5).fit(X, y)\n",
    "t_ridge_cv = time.time() - t1\n",
    "\n",
    "RIDGEdic[\"coefficients\"]=model.coef_\n",
    "RIDGEdic[\"alpha\"]=model.alpha_\n",
    "RIDGEdic[\"intercept\"]=model.intercept_\n",
    "\n",
    "RIDGEdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfRI=ln.Ridge(alpha=RIDGEdic[\"alpha\"])\n",
    "clfRI.fit(X,y)\n",
    "clfRI.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ElasticNet: parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys=[\"alpha\",\"l1ratio\",\"intercept\",\"coefficients\"]\n",
    "ELNETdic=dict.fromkeys(keys,None)\n",
    "print(\"Computing regularization path using the coordinate descent elasticnet...\")\n",
    "t1 = time.time()\n",
    "model = ln.ElasticNetCV(cv=5).fit(X, y)\n",
    "t_lasso_cv = time.time() - t1\n",
    "\n",
    "# Display results\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "#ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: CV estimate')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.title('Mean square error on each fold: coordinate descent '\n",
    "          '(train time: %.2fs)' % t_lasso_cv)\n",
    "plt.axis('tight')\n",
    "#plt.ylim(ymin, ymax)\n",
    "\n",
    "ELNETdic[\"alpha\"]=model.alpha_\n",
    "ELNETdic[\"coefficients\"]=model.coef_\n",
    "ELNETdic[\"l1ratio\"]=model.l1_ratio_\n",
    "ELNETdic[\"intercept\"]=model.intercept_\n",
    "\n",
    "ELNETdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfEL=ln.ElasticNet(alpha=ELNETdic[\"alpha\"],l1_ratio=ELNETdic[\"l1ratio\"])\n",
    "clfEL.fit(X,y)\n",
    "clfEL.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: tuning number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "scoresk = list()\n",
    "y=train[target]\n",
    "n_folds =5\n",
    "to_plot=[]\n",
    "times=[]\n",
    "for i in range(1,30):\n",
    "    t1 = time.time()\n",
    "    clf = KNeighborsRegressor(n_neighbors=i,weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski')\n",
    "    this_scores = cross_val_score(clf, X, y, cv=n_folds, n_jobs=-1)\n",
    "    t_train=time.time() - t1\n",
    "    times.append(t_train)\n",
    "    scoresk.append([i,np.mean(this_scores),np.std(this_scores)])\n",
    "    to_plot.append(list(this_scores))\n",
    "\n",
    "optk=max(scoresk,key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting our tuning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
